Building DAG of jobs...
Using shell: /bin/bash
Provided cluster nodes: 1
Conda environments: ignored
Job stats:
job                 count    min threads    max threads
----------------  -------  -------------  -------------
iterative_blastp        1              1              1
total                   1              1              1

Select jobs to execute...

[Fri Apr 21 12:31:30 2023]
localrule iterative_blastp:
    input: initial_proteins/ORF6_initial_proteins2.faa
    output: blastp_proteins/ORF6_blastp_proteins.faa
    log: log/ORF6_test.log
    jobid: 0
    wildcards: ORF=ORF6
    resources: tmpdir=/tmp, mem=50GB, threads=4, time=10:00:00, partition=CLUSTER

[Fri Apr 21 12:31:31 2023]
Error in rule iterative_blastp:
    jobid: 0
    output: blastp_proteins/ORF6_blastp_proteins.faa
    log: log/ORF6_test.log (check log file(s) for error message)
    shell:
        
        mkdir blastp2_proteins
        python3 scripts/01_iterative_blast.py         initial_proteins/ORF6_initial_proteins2.faa blastp_proteins/ORF6_blastp_proteins.faa --db initial_proteins/test.dmnd         --evalue 1e-05         --bitscore 50 
        
        (one of the commands exited with non-zero exit code; note that snakemake uses bash strict mode!)

Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /home/dlueckin/projects/pR1SE_australian_lakes/generate_protein_clusters/.snakemake/log/2023-04-21T123129.464210.snakemake.log
